{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ff4a71bb0ceecbc818a759107c5c11c91ef8c85"
   },
   "source": [
    "![](https://bitcoinist.com/wp-content/uploads/2018/06/shutterstock_1018654609.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Goal of this kernel is to compare NN and ARIMA modelling. We will be predicting Bitcoin prices with help of Bitcoin historical data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "**There are 4 csv files. CSV files for select bitcoin exchanges for the time period of Jan 2012 to July 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields forward filled from the last valid time period. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. **\n",
    "\n",
    "coincheckJPY_1-min_data_2014-10-31_to_2018-06-27.csv\n",
    "\n",
    "bitflyerJPY_1-min_data_2017-07-04_to_2018-06-27.csv\n",
    "\n",
    "coinbaseUSD_1-min_data_2014-12-01_to_2018-06-27.csv\n",
    "\n",
    "bitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv\n",
    "\n",
    "**All from different Bitcoin exchanges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "794936030db685ac980a2a6f84c6196d26f6f5b4"
   },
   "source": [
    "**RNN** To predict bitcoin prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "fe2f4a4d672d8fffc8569633ecde136b9b78fba3"
   },
   "outputs": [],
   "source": [
    "# First step, import libraries and then dataset\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "f8b0d7ec59102288537c49b56a4cad48d8d19f74"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv' does not exist: b'../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f3af6b9e593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the dataset and encode the date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mReal_Price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weighted_Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv' does not exist: b'../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv'"
     ]
    }
   ],
   "source": [
    "# Import the dataset and encode the date\n",
    "df = pd.read_csv(\"../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv\")\n",
    "df['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\n",
    "group = df.groupby('date')\n",
    "Real_Price = group['Weighted_Price'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75816fc71b5114991678b59ab03e468b44c78e1b"
   },
   "source": [
    "Bitcoin predictions are going to be for a month, that is why we need to split the dataset accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1acf607516505a536a4432d99e1e0c114edc1b5"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "prediction_days = 30\n",
    "df_train= Real_Price[:len(Real_Price)-prediction_days]\n",
    "df_test= Real_Price[len(Real_Price)-prediction_days:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cbcda5a498a5b2d5a73f858cf5e00647d14d6b0"
   },
   "source": [
    "Some pre-processing is also necessary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f67e6eb9b0bdfab4f23040ff37e054eb3e40ee38"
   },
   "outputs": [],
   "source": [
    "# Data preprocess\n",
    "training_set = df_train.values\n",
    "training_set = np.reshape(training_set, (len(training_set), 1))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_set = sc.fit_transform(training_set)\n",
    "X_train = training_set[0:len(training_set)-1]\n",
    "y_train = training_set[1:len(training_set)]\n",
    "X_train = np.reshape(X_train, (len(X_train), 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5838cc546f91700965eb078dd03d2d33003120cb"
   },
   "source": [
    "Now keras  to build the rNN, Long short-term memory!!! LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68354ae10f922d378e1d887b54cdd78dec78ae84"
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 4, activation = 'sigmoid', input_shape = (None, 1)))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, batch_size = 5, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f161a3181047b949b80bcf045704a13aaf4fa0c"
   },
   "source": [
    "**NOTE!!!** Key thing is following and that is why NN COULD \"fail\". WE used values of today to predict the future values. That is not really failure of NN but jsut goes to show that we need to think about what are we \"feeding\" our NN with. Because it can happen that NN will only learn that price will be slightly higher than yesterdays price. Which is true, except when it is not. Than we fail big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "406cf62e82c9c09bc47a9cb3cb3541590b8318d0"
   },
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "test_set = df_test.values\n",
    "inputs = np.reshape(test_set, (len(test_set), 1))\n",
    "inputs = sc.transform(inputs)\n",
    "inputs = np.reshape(inputs, (len(inputs), 1, 1))\n",
    "predicted_BTC_price = regressor.predict(inputs)\n",
    "predicted_BTC_price = sc.inverse_transform(predicted_BTC_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fc5d83199f99442778ca89d05720962e3001ad7"
   },
   "outputs": [],
   "source": [
    "# Visualising the results\n",
    "plt.figure(figsize=(25,15), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = plt.gca()  \n",
    "plt.plot(test_set, color = 'red', label = 'Real BTC Price')\n",
    "plt.plot(predicted_BTC_price, color = 'blue', label = 'Predicted BTC Price')\n",
    "plt.title('BTC Price Prediction', fontsize=40)\n",
    "df_test = df_test.reset_index()\n",
    "x=df_test.index\n",
    "labels = df_test['date']\n",
    "plt.xticks(x, labels, rotation = 'vertical')\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(18)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(18)\n",
    "plt.xlabel('Time', fontsize=40)\n",
    "plt.ylabel('BTC Price(USD)', fontsize=40)\n",
    "plt.legend(loc=2, prop={'size': 25})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a1a39899c2dfece8d3e8fca4c74d92bb4eeb3f4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82c77aef7e63988f53571fb17af0ad35f0921a41"
   },
   "source": [
    "**ARIMA** Let us first go through theoretical part of ARIMA. (NN should be already familiar, if not visit my other kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80110939f554828388e6baddfd0c0dd300d2bfdc"
   },
   "source": [
    "An **ARIMA** model is a class of statistical models for analyzing and forecasting time series data. ARIMA model is one model for non-stationarity. It assumes that the data becomes stationary after differencing.\n",
    "\n",
    "**ARIMA** is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n",
    "\n",
    "\n",
    "\n",
    "These acronyms describe it pretty well:\n",
    "1. **AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
    "2.**I**: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
    "3. **MA**: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "\n",
    "\n",
    "\n",
    "Each of these components are explicitly specified in the model as a parameter. A standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n",
    "\n",
    "Parameters are defined as follows:\n",
    "\n",
    "1. **p**: The number of lag observations included in the model, also called the lag order.\n",
    "2. **d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "3. **q**: The size of the moving average window, also called the order of moving average.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81c4aa3b2ddd5c6a093af0f88f23dfae3f9274f0"
   },
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Adopting an ARIMA model for a time series assumes that the underlying process that generated the observations is an ARIMA process. This may seem obvious, but helps to motivate the need to confirm the assumptions of the model in the raw observations and in the residual errors of forecasts from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1c62bd535618848bfbdb7fcca0c2b0f8d24e23f"
   },
   "source": [
    "But how do we check that? And how to de determine the parameters p,d,q in the model?\n",
    "First of all we need to make sure that the time-series is stationary, thats where differencing comes into place (degree corrects the level of non-stationarity if possible) And model parameters can be determined with the Box-Jenkins Method.\n",
    "\n",
    "Basicaly we have the following situation:\n",
    "1. Define the model by calling ARIMA() and passing in the p, d, and q parameters.\n",
    "2. The model is prepared on the training data by calling the fit() function.\n",
    "3. Predictions can be made by calling the predict() function and specifying the index of the time or times to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8388d50b20f2796bc198faa7371f07e19dba792"
   },
   "source": [
    "How does **Box-Jenkins Method** work?\n",
    "[https://machinelearningmastery.com/gentle-introduction-box-jenkins-method-time-series-forecasting/](http://)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8c4c1f68774520cc54384f6e834ca093824b44bb"
   },
   "source": [
    "Let us start coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3a99c0863aefa585d4e96090269c123b7d19646"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d23167cc675fce980d145509359e7030295b86b1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../input/coinbaseUSD_1-min_data_2014-12-01_to_2018-11-11.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "826c21a5b2a68b6514a2b603f7cac149c0d25567"
   },
   "source": [
    "We need to transform our index into time data and then split the time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb9db2a75b46598968ec3874db742dfbf0b1dd71"
   },
   "outputs": [],
   "source": [
    "# Unix-time to \n",
    "df.Timestamp = pd.to_datetime(df.Timestamp, unit='s')\n",
    "\n",
    "# Resampling to daily frequency\n",
    "df.index = df.Timestamp\n",
    "df = df.resample('D').mean()\n",
    "\n",
    "# Resampling to monthly frequency\n",
    "df_month = df.resample('M').mean()\n",
    "\n",
    "# Resampling to annual frequency\n",
    "df_year = df.resample('A-DEC').mean()\n",
    "\n",
    "# Resampling to quarterly frequency\n",
    "df_Q = df.resample('Q-DEC').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5ee2826751fb6e1432fbd51d4072e489cb4e49d"
   },
   "source": [
    "Visualize the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "374a3256d727b011f6b256e97a7f71958c5f8433"
   },
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "fig = plt.figure(figsize=[15, 7])\n",
    "plt.suptitle('Bitcoin exchanges, mean USD', fontsize=22)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(df.Weighted_Price, '-', label='By Days')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(df_month.Weighted_Price, '-', label='By Months')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(df_Q.Weighted_Price, '-', label='By Quarters')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(df_year.Weighted_Price, '-', label='By Years')\n",
    "plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ccf91e4ba1372d1596ebb4c3b20e9ac7cf9ea89"
   },
   "source": [
    "**Stationarity check and STL-decomposition of the series*** Lower the p value the better. Stationarity is our models main assumption and dickey fuller is just hypothesis test of the unit root test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8468d27b5ffcbc4005e2b48aee7dd4dd2fe40d11"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,7])\n",
    "sm.tsa.seasonal_decompose(df_month.Weighted_Price).plot()\n",
    "print(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.Weighted_Price)[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cee7a1d09c18f0fcb7d42cd9f9db3e52fa2fb62"
   },
   "source": [
    "Obviously not stationary, hence we ought transform our data. First Box-cox transformation then check the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "926c1613f39ab3155108e59be0ca427c50754024"
   },
   "outputs": [],
   "source": [
    "# Box-Cox Transformations\n",
    "df_month['Weighted_Price_box'], lmbda = stats.boxcox(df_month.Weighted_Price)\n",
    "print(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.Weighted_Price)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "10f15f389aa16851bc4541c37841fd392ab47d97"
   },
   "source": [
    "We need another transformation. Seasonal differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de97769ac38c8cecaa6e1440659e9c1af6eead4c"
   },
   "outputs": [],
   "source": [
    "# Seasonal differentiation\n",
    "df_month['prices_box_diff'] = df_month.Weighted_Price_box - df_month.Weighted_Price_box.shift(12)\n",
    "print(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.prices_box_diff[12:])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f4cf9ff3abdeced446e71f90988c87f7c087804"
   },
   "source": [
    "Again series is not stationary, finally let us try regular differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f3b844f3fd7b63812f45d2d722ee753c1fd2630"
   },
   "outputs": [],
   "source": [
    "# Regular differentiation\n",
    "df_month['prices_box_diff2'] = df_month.prices_box_diff - df_month.prices_box_diff.shift(1)\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "# STL-decomposition\n",
    "sm.tsa.seasonal_decompose(df_month.prices_box_diff2[13:]).plot()   \n",
    "print(\"Dickey–Fuller test: p=%f\" % sm.tsa.stattools.adfuller(df_month.prices_box_diff2[13:])[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1a86ef6e88d1201e7e9f9c45cdf017e114c0018"
   },
   "source": [
    "Now we need to make model selection, with help of :\n",
    "* Autocorrelation Function (ACF). The plot summarizes the correlation of an observation with lag values. The x-axis shows the lag and the y-axis shows the correlation coefficient between -1 and 1 for negative and positive correlation.\n",
    "* Partial Autocorrelation Function (PACF). The plot summarizes the correlations for an observation with lag values that is not accounted for by prior lagged observations.\n",
    "We can get a basic picture of the parameter interval, and using this heuristic we can  with help of AIC- Akaike information criterion decide which are the best p,q,d for ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76c114e52fd8a50809f944d18ce6f440c2bb181a"
   },
   "outputs": [],
   "source": [
    "# Initial approximation of parameters\n",
    "Qs = range(0, 2)\n",
    "qs = range(0, 3)\n",
    "Ps = range(0, 3)\n",
    "ps = range(0, 3)\n",
    "D=1\n",
    "d=1\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "len(parameters_list)\n",
    "\n",
    "# Model Selection\n",
    "results = []\n",
    "best_aic = float(\"inf\")\n",
    "warnings.filterwarnings('ignore')\n",
    "for param in parameters_list:\n",
    "    try:\n",
    "        model=sm.tsa.statespace.SARIMAX(df_month.Weighted_Price_box, order=(param[0], d, param[1]), \n",
    "                                        seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n",
    "    except ValueError:\n",
    "        print('wrong parameters:', param)\n",
    "        continue\n",
    "    aic = model.aic\n",
    "    if aic < best_aic:\n",
    "        best_model = model\n",
    "        best_aic = aic\n",
    "        best_param = param\n",
    "    results.append([param, model.aic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33052257cef491e8fd11784255f7b6d10b56e566"
   },
   "outputs": [],
   "source": [
    "# Best Models\n",
    "result_table = pd.DataFrame(results)\n",
    "result_table.columns = ['parameters', 'aic']\n",
    "print(result_table.sort_values(by = 'aic', ascending=True).head())\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17a71f38f4781ce5e6228ff97f8f94c4dedef7df"
   },
   "source": [
    "Good, now we can make predictions with our (ARIMA) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a3580483132a7500f8321b49dfca64d2b498eb3"
   },
   "outputs": [],
   "source": [
    "# Inverse Box-Cox Transformation Function\n",
    "def invboxcox(y,lmbda):\n",
    "   if lmbda == 0:\n",
    "      return(np.exp(y))\n",
    "   else:\n",
    "      return(np.exp(np.log(lmbda*y+1)/lmbda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfa67cfbd748f30b062feafee83053724769db89"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "df_month2 = df_month[['Weighted_Price']]\n",
    "date_list = [datetime(2017, 6, 30), datetime(2017, 7, 31), datetime(2017, 8, 31), datetime(2017, 9, 30), \n",
    "             datetime(2017, 10, 31), datetime(2017, 11, 30), datetime(2017, 12, 31), datetime(2018, 1, 31),\n",
    "             datetime(2018, 1, 28)]\n",
    "future = pd.DataFrame(index=date_list, columns= df_month.columns)\n",
    "df_month2 = pd.concat([df_month2, future])\n",
    "df_month2['forecast'] = invboxcox(best_model.predict(start=0, end=75), lmbda)\n",
    "plt.figure(figsize=(15,7))\n",
    "df_month2.Weighted_Price.plot()\n",
    "df_month2.forecast.plot(color='r', ls='--', label='Predicted Weighted_Price')\n",
    "plt.legend()\n",
    "plt.title('Bitcoin exchanges, by months')\n",
    "plt.ylabel('mean USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69c2dd785ae465bf7a412f7d68787055d950a141"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e18dfdd6049c089909075a6f33773a48d6e836bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01b72aeb5d1973f5711277e0fcb9f487b2f10988"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b7af6ceaac00dff6c13fc89d7c3b02360fc5328"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d7959b134cd1f8f882ae55d25bf4a6d42b2eb5f"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e1a7edeb828ab4d133e7b4b625d8d356d1208f0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b6b94e2c82fde34ed865464b818d6c9c48e8da2"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
